{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ramsay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Ramsay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download('words')\n",
    "stop_english = set(stopwords.words('english'))\n",
    "english_words = set(x.lower() for x in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df,field):\n",
    "\tprint(\"start cleaning\")\n",
    "\tdf[field] = df[field].str.replace('[^a-zA-Z ]', '')\n",
    "\tdf[field].str.lower()\n",
    "\tdf = df.dropna(how='all')\n",
    "\tprint(\"cleaning done\")\n",
    "\treturn df\n",
    "\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "\ttokens = sentence.split()\n",
    "\t#Drop stopwords\n",
    "\tstemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\treturn ' '.join(stemmed_tokens)\n",
    "\n",
    "def stem_df(df,field,language): \n",
    "\tprint(\"Stemming\")\n",
    "\t\n",
    "\tglobal stemmer\n",
    "\tstemmer = SnowballStemmer(language)\n",
    "\tdf[field] = df[field].apply(stem_sentences)\n",
    "\t\n",
    "\treturn df\n",
    "    \n",
    "def drop_stop_df(df,field):\n",
    "\tprint(\"Dropping stop\")\n",
    "\t\n",
    "\tdf[field] = df[field].apply(drop_stop_sentence)\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def drop_stop_sentence(sentence):\n",
    "\ttokens = sentence.split()\n",
    "\tstopped_tokens = [x for x in tokens if x not in stop_english]\n",
    "\treturn ' '.join(stopped_tokens)\n",
    "\n",
    "def remove_english(df,field):\n",
    "\tprint(\"Dropping english\")\n",
    "\tdf[field] = df[field].apply(drop_eng_sentence)\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def drop_eng_sentence(sentence):\n",
    "\ttokens = sentence.split()\n",
    "\tstopped_tokens = [x for x in tokens if x not in english_words]\n",
    "\treturn ' '.join(stopped_tokens)\n",
    "\n",
    "def remove_non_english(df,field):\n",
    "\tprint(\"Dropping english\")\n",
    "\tdf[field] = df[field].apply(drop_non_eng_sentence)\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def drop_non_eng_sentence(sentence):\n",
    "\ttokens = sentence.split()\n",
    "\tstopped_tokens = [x for x in tokens if x in english_words]\n",
    "\treturn ' '.join(stopped_tokens)\n",
    "\n",
    "def apply_tfidf(verbs):\n",
    "    tfidf = TfidfVectorizer(sublinear_tf= True,\n",
    "                            min_df= 3,\n",
    "                            norm= 'l2',\n",
    "                            encoding= 'latin-1',\n",
    "                            ngram_range= (1,2),\n",
    "                            stop_words= 'english',\n",
    "#                             max_features = 40000,\n",
    "                           )\n",
    "    features = tfidf.fit_transform(verbs)\n",
    "    \n",
    "    return features, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(features, target, target_name, conf):\n",
    "            try:\n",
    "                h2o.cluster().shutdown()\n",
    "            except:\n",
    "                pass\n",
    "            h2o.init(max_mem_size=200)\n",
    "            print(\"Train data size : {}, targets size :{}\".format(features.shap, target.shape))\n",
    "            print(\"Targets : {}\".format(target.unique()))\n",
    "            target = target.astype('category')\n",
    "            target = target.cat.codes\n",
    "            obs = target.shape[0]\n",
    "\n",
    "            if obs >= conf['min_train_records'] and len(target.unique()) > 1:\n",
    "                feature_list = features.columns\n",
    "                features[target_name] = target\n",
    "                \n",
    "                # build model on training data\n",
    "                print(features)\n",
    "                train_data_h = h2o.H2OFrame(features)\n",
    "                train_data_h[target_name] = train_data_h[target_name].asfactor()\n",
    "                training_types = (train_data_h.types)\n",
    "\n",
    "                y = target_name\n",
    "                x = list(feature_list)\n",
    "                \n",
    "                # Partition data into 80% ,20% chunks\n",
    "                splits = train_data_h.split_frame(ratios=[conf['train_perc']], seed=1)\n",
    "                train = splits[0]\n",
    "                valid = splits[1]\n",
    "                \n",
    "                print(\"AutoML Starting \")\n",
    "                try:\n",
    "                    aml = H2OAutoML(max_models = int(conf['max_models_fit']),\n",
    "                                    max_runtime_secs = conf['max_model_runtime'],\n",
    "                                    seed = 1234,\n",
    "                                    stopping_metric = conf['stopping_measure'],\n",
    "                                    balance_classes = True,\n",
    "                                    nfolds=conf['nfolds'],\n",
    "                                    include_algos = conf['include_algos'])\n",
    "\n",
    "                    aml.train(  x = x,\n",
    "                                y = y,\n",
    "                                training_frame = train,\n",
    "                                validation_frame = valid)\n",
    "\n",
    "                    ###Uncomment below to enable scoring\n",
    "                    model_path = conf['model_folder']\n",
    "\n",
    "                    model_name = h2o.save_model(aml.leader,\n",
    "                                                path = model_path,\n",
    "                                                force = True)\n",
    "\n",
    "\n",
    "                    # c_w.write(f'ndicate/model/dtypes_{aml.leader.model_id}.json',training_types,'processing')  ##USED WHERE MODEL HAS SPECIFIC DTYPES PER FEATURE\n",
    "\n",
    "                    print(f\"\\n\\n\\t\\t\\t\\t\\t{aml.leader.model_id}\\n\\n\")\n",
    "\n",
    "                    return True\n",
    "                except Exception as E:\n",
    "                    print(f\"Failiure Training for {target_name} \")\n",
    "                    print(E)\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"Not enough training records to build: {target_name}\")\n",
    "                return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "0: Ok lar... Joking wif u oni...\n",
      "\n",
      "1: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "0: U dun say so early hor... U c already then say...\n",
      "\n",
      "0: Nah I don't think he goes to usf, he lives around here though\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Format Data\n",
    "f = open(\"data/smsspamcollection/SMSSpamCollection\", \"r\")\n",
    "targets = []\n",
    "texts = []\n",
    "for line in f:\n",
    "    columns = line.split(\"\\t\", 1)\n",
    "    texts.append(columns[1])\n",
    "    if columns[0] == \"ham\":\n",
    "        targets.append(0)\n",
    "    elif columns[0] == \"spam\":\n",
    "        targets.append(1)\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        break\n",
    "# for i in range(5):\n",
    "#     print(\"{}: {}\".format(targets[i], texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  Go until jurong point, crazy.. Available only ...\n",
       "1       0                    Ok lar... Joking wif u oni...\\n\n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       0  U dun say so early hor... U c already then say...\n",
       "4       0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"target\"] = targets\n",
    "df[\"text\"] = texts\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start cleaning\n",
      "cleaning done\n",
      "Dropping english\n",
      "Dropping stop\n",
      "Stemming\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>point crazi n great world la e buffet got wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>lar u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>entri win final st receiv entri appli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dun say earli c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>dont think goe around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                           text\n",
       "0       0  point crazi n great world la e buffet got wat\n",
       "1       0                                          lar u\n",
       "2       1          entri win final st receiv entri appli\n",
       "3       0                    dun say earli c alreadi say\n",
       "4       0                   dont think goe around though"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field = \"text\"\n",
    "text_only = clean(df, text_field)\n",
    "text_only = remove_non_english(text_only, text_field)\n",
    "dropped_stop_text = drop_stop_df(text_only, text_field)\n",
    "stemmed_text = stem_df(dropped_stop_text, text_field, \"english\")\n",
    "\n",
    "stemmed_text.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"text_field\": \"text\",\n",
    "#     \"id_fields\" : [BI_ID, audio_id],\n",
    "\t\"segment\": True,\n",
    "\t\"low_memory\": False,\n",
    "    \"train_perc\": 0.9,\n",
    "\t\"min_retrain_cnt\": 50,\n",
    "\t\"min_train_records\": 10, \n",
    "    \"train_perc\": 0.8,\n",
    "    \"model_train_start_date\": \"1900-01-01\",\n",
    "    \"max_models_fit\": 5,\n",
    "    \"max_model_runtime\": 3600,\n",
    "    \"nfolds\": 0,\n",
    "    \"accuracy_measure\": \"AUC\",\n",
    "    \"stopping_measure\": \"AUC\",\n",
    "    \"include_algos\": [\"GLM\", \"DRF\", \"GBM\"],\n",
    "    \"exclude_algos\": [\"StackedEnsemble\", \"XGBoost\", \"DeepLearning\"],\n",
    "    \"model_folder\": \"/data/models/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training for Spam\n",
      "H2O session _sid_b1b7 closed.\n",
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; OpenJDK 64-Bit Server VM 18.9 (build 11.0.2+9, mixed mode)\n",
      "  Starting server from c:\\users\\ramsay\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\Ramsay\\AppData\\Local\\Temp\\tmpoby2_ba_\n",
      "  JVM stdout: C:\\Users\\Ramsay\\AppData\\Local\\Temp\\tmpoby2_ba_\\h2o_Ramsay_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\Ramsay\\AppData\\Local\\Temp\\tmpoby2_ba_\\h2o_Ramsay_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is too old (10 months and 18 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>04 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Africa/Harare</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.26.0.10</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>10 months and 18 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_Ramsay_xsl0me</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>200 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>{'http': None, 'https': None}</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.1 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------------\n",
       "H2O cluster uptime:         04 secs\n",
       "H2O cluster timezone:       Africa/Harare\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.26.0.10\n",
       "H2O cluster version age:    10 months and 18 days !!!\n",
       "H2O cluster name:           H2O_from_python_Ramsay_xsl0me\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    200 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:       {'http': None, 'https': None}\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.6.1 final\n",
       "--------------------------  ---------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size : (5574, 2018), targets size :(5574,)\n",
      "Targets : [0 1]\n",
      "        0    1    2    3    4    5    6    7    8    9  ...  2008  2009  2010  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "6     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "7     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "8     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "10    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "11    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "12    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "13    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "14    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "15    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "16    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "17    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "18    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "19    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "20    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "21    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "22    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "23    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "24    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "25    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "26    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "27    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "28    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "29    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   \n",
      "5544  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5545  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5546  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5547  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5548  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5549  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5550  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5551  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5552  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5553  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5554  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5555  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5556  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5557  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5558  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5559  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5560  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5561  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5562  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5563  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5564  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5565  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5566  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5567  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5568  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5569  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5570  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5571  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5572  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "5573  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
      "\n",
      "      2011  2012  2013  2014  2015  target  \\\n",
      "0      0.0   0.0   0.0   0.0   0.0       0   \n",
      "1      0.0   0.0   0.0   0.0   0.0       0   \n",
      "2      0.0   0.0   0.0   0.0   0.0       1   \n",
      "3      0.0   0.0   0.0   0.0   0.0       0   \n",
      "4      0.0   0.0   0.0   0.0   0.0       0   \n",
      "5      0.0   0.0   0.0   0.0   0.0       1   \n",
      "6      0.0   0.0   0.0   0.0   0.0       0   \n",
      "7      0.0   0.0   0.0   0.0   0.0       0   \n",
      "8      0.0   0.0   0.0   0.0   0.0       1   \n",
      "9      0.0   0.0   0.0   0.0   0.0       1   \n",
      "10     0.0   0.0   0.0   0.0   0.0       0   \n",
      "11     0.0   0.0   0.0   0.0   0.0       1   \n",
      "12     0.0   0.0   0.0   0.0   0.0       1   \n",
      "13     0.0   0.0   0.0   0.0   0.0       0   \n",
      "14     0.0   0.0   0.0   0.0   0.0       0   \n",
      "15     0.0   0.0   0.0   0.0   0.0       1   \n",
      "16     0.0   0.0   0.0   0.0   0.0       0   \n",
      "17     0.0   0.0   0.0   0.0   0.0       0   \n",
      "18     0.0   0.0   0.0   0.0   0.0       0   \n",
      "19     0.0   0.0   0.0   0.0   0.0       1   \n",
      "20     0.0   0.0   0.0   0.0   0.0       0   \n",
      "21     0.0   0.0   0.0   0.0   0.0       0   \n",
      "22     0.0   0.0   0.0   0.0   0.0       0   \n",
      "23     0.0   0.0   0.0   0.0   0.0       0   \n",
      "24     0.0   0.0   0.0   0.0   0.0       0   \n",
      "25     0.0   0.0   0.0   0.0   0.0       0   \n",
      "26     0.0   0.0   0.0   0.0   0.0       0   \n",
      "27     0.0   0.0   0.0   0.0   0.0       0   \n",
      "28     0.0   0.0   0.0   0.0   0.0       0   \n",
      "29     0.0   0.0   0.0   0.0   0.0       0   \n",
      "...    ...   ...   ...   ...   ...     ...   \n",
      "5544   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5545   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5546   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5547   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5548   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5549   0.0   0.0   0.0   0.0   0.0       1   \n",
      "5550   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5551   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5552   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5553   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5554   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5555   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5556   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5557   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5558   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5559   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5560   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5561   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5562   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5563   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5564   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5565   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5566   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5567   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5568   0.0   0.0   0.0   0.0   0.0       1   \n",
      "5569   0.0   0.0   0.0   0.0   0.0       1   \n",
      "5570   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5571   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5572   0.0   0.0   0.0   0.0   0.0       0   \n",
      "5573   0.0   0.0   0.0   0.0   0.0       0   \n",
      "\n",
      "                                                   text  \n",
      "0         point crazi n great world la e buffet got wat  \n",
      "1                                                 lar u  \n",
      "2                 entri win final st receiv entri appli  \n",
      "3                           dun say earli c alreadi say  \n",
      "4                          dont think goe around though  \n",
      "5                 darl word back like fun still ok send  \n",
      "6                  brother like speak treat like patent  \n",
      "7                                  per request set copi  \n",
      "8     valu network custom select prize reward claim ...  \n",
      "9                            mobil latest colour camera  \n",
      "10    home soon dont want talk stuff tonight k cri e...  \n",
      "11                                   win send day appli  \n",
      "12                                 week membership word  \n",
      "13    search right thank breather promis wont take h...  \n",
      "14                                                       \n",
      "15              use credit click link next messag click  \n",
      "16                                            kim watch  \n",
      "17             u rememb spell name v naughti make v wet  \n",
      "18                                that way u feel way b  \n",
      "19                      v dont miss news ur nation team  \n",
      "20                                   serious spell name  \n",
      "21                                         go tri ha ha  \n",
      "22                               pay first lar da stock  \n",
      "23              finish lunch go finish ur lunch alreadi  \n",
      "24                                      way meet sooner  \n",
      "25    forc eat slice realli hungri tho get worri sic...  \n",
      "26                                        alway convinc  \n",
      "27          catch bus egg make tea eat left dinner feel  \n",
      "28                         back car let know there room  \n",
      "29                                vagu rememb feel like  \n",
      "...                                                 ...  \n",
      "5544                                    get ass epsilon  \n",
      "5545                         still havent got jacket ah  \n",
      "5546  take derek taylor back time done leav mous des...  \n",
      "5547                                durban still number  \n",
      "5548                                              lotta  \n",
      "5549                               contract mobil remov  \n",
      "5550                                        tri weekend  \n",
      "5551                know wot peopl wear hat belt know r  \n",
      "5552                                     time think get  \n",
      "5553                             get spiritu deep great  \n",
      "5554                 safe trip happi soon compani share  \n",
      "5555                                         brain dear  \n",
      "5556  keep mind got enough gas one round trip bar su...  \n",
      "5557  nice bit go drink sometim soon go da work laug...  \n",
      "5558                                        that u much  \n",
      "5559  meant calcul school realli expens accent impor...  \n",
      "5560                                         call later  \n",
      "5561                                    arent next flip  \n",
      "5562                                                 us  \n",
      "5563                               dump heap decid come  \n",
      "5564                   salesman ask say quit use consid  \n",
      "5565                                               like  \n",
      "5566              dont wait til least wednesday see get  \n",
      "5567                                                lei  \n",
      "5568  get free call credit great repli text valid na...  \n",
      "5569  time tri contact u prize claim easi call p per...  \n",
      "5570                                 b go esplanad home  \n",
      "5571                                               mood  \n",
      "5572  guy like id interest someth els next week gave...  \n",
      "5573                                          true name  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5574 rows x 2018 columns]\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML Starting \n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n",
      "\n",
      "\n",
      "\t\t\t\t\tGLM_grid_1_AutoML_20200925_131508_model_1\n",
      "\n",
      "\n",
      "Completed Training\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting Training for Spam\")\n",
    "train_df = stemmed_text.loc[~stemmed_text[\"target\"].isna()]\n",
    "features, tfidf = apply_tfidf(train_df[conf['text_field']])\n",
    "tfidf_df = pd.DataFrame(features.todense())\n",
    "if tfidf_df.shape[0] != train_df.shape[0]:\n",
    "    print(\"Mis match on training data sizes\")\n",
    "    print(\"TFIDF Shape {}\".format(tfidf_df.shape))\n",
    "    print(\"Train Shape {}\".format(train_df.shape))\n",
    "else:\n",
    "    features_df = pd.merge(tfidf_df, train_df.reset_index(drop=True), left_index=True, right_index=True).copy()  \n",
    "    features_df.columns = features_df.columns.astype(str)\n",
    "    training_completed = build(features_df, train_df[\"target\"], \"target\", conf)\n",
    "    if training_completed:   \n",
    "        print(\"Completed Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
