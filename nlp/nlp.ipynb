{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ramsay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Ramsay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import ssl\n",
    "# import os\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import nltk\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('words')\n",
    "stop_english = set(stopwords.words('english'))\n",
    "#English corpus added and functions to remove english/non english\n",
    "english_words = set(x.lower() for x in nltk.corpus.words.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df,field):\n",
    "\tprint(\"start cleaning\")\n",
    "\tdf[field] = df[field].str.replace('[^a-zA-Z ]', '')\n",
    "\tdf[field].str.lower()\n",
    "\tdf = df.dropna(how='all')\n",
    "\tprint(\"cleaning done\")\n",
    "\treturn df\n",
    "\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "\ttokens = sentence.split()\n",
    "\t#Drop stopwords\n",
    "\tstemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\treturn ' '.join(stemmed_tokens)\n",
    "\n",
    "def stem_df(df,field,language): \n",
    "\tprint(\"Stemming\")\n",
    "\t\n",
    "\tglobal stemmer\n",
    "\tstemmer = SnowballStemmer(language)\n",
    "\tdf[field] = df[field].apply(stem_sentences)\n",
    "\t\n",
    "\treturn df\n",
    "    \n",
    "def drop_stop_df(df,field):\n",
    "\tprint(\"Dropping stop\")\n",
    "\t\n",
    "\tdf[field] = df[field].apply(drop_stop_sentence)\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def drop_stop_sentence(sentence):\n",
    "\ttokens = sentence.split()\n",
    "\tstopped_tokens = [x for x in tokens if x not in stop_english]\n",
    "\treturn ' '.join(stopped_tokens)\n",
    "\n",
    "def remove_english(df,field):\n",
    "\tprint(\"Dropping english\")\n",
    "\tdf[field] = df[field].apply(drop_eng_sentence)\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def drop_eng_sentence(sentence):\n",
    "\ttokens = sentence.split()\n",
    "\tstopped_tokens = [x for x in tokens if x not in english_words]\n",
    "\treturn ' '.join(stopped_tokens)\n",
    "\n",
    "def remove_non_english(df,field):\n",
    "\tprint(\"Dropping english\")\n",
    "\tdf[field] = df[field].apply(drop_non_eng_sentence)\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "def drop_non_eng_sentence(sentence):\n",
    "\ttokens = sentence.split()\n",
    "\tstopped_tokens = [x for x in tokens if x in english_words]\n",
    "\treturn ' '.join(stopped_tokens)\n",
    "\n",
    "def apply_tfidf(verbs):\n",
    "    tfidf = TfidfVectorizer(sublinear_tf= True,\n",
    "                            min_df= 3,\n",
    "                            norm= 'l2',\n",
    "                            encoding= 'latin-1',\n",
    "                            ngram_range= (1,2),\n",
    "                            stop_words= 'english',\n",
    "#                             max_features = 40000,\n",
    "                           )\n",
    "    features = tfidf.fit_transform(verbs)\n",
    "    \n",
    "    return features, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(features, target, target_name, conf):\n",
    "            try:\n",
    "                h2o.cluster().shutdown()\n",
    "            except:\n",
    "                pass\n",
    "            h2o.init(max_mem_size=200)\n",
    "            print(f\"Train data size : {features.shape}, targets size :{target.shape}\")\n",
    "            print(f\"Targets : {target.unique()}\")\n",
    "            target= target.astype('category')\n",
    "            target= target.cat.codes\n",
    "            obs = target.shape[0]\n",
    "\n",
    "            if obs>=conf['min_train_records'] and len(target.unique())>1:\n",
    "                feature_list= features.columns\n",
    "                features[target_name] = target\n",
    "                \n",
    "                # build model on training data\n",
    "                print(features)\n",
    "                train_data_h = h2o.H2OFrame(features)\n",
    "                train_data_h[target_name] = train_data_h[target_name].asfactor()\n",
    "                training_types = (train_data_h.types)\n",
    "\n",
    "                y = target_name\n",
    "                x = list(feature_list)\n",
    "                \n",
    "                # Partition data into 80% ,20% chunks\n",
    "                splits = train_data_h.split_frame(ratios=[conf['train_perc']], seed=1)\n",
    "                train = splits[0]\n",
    "                valid = splits[1]\n",
    "                \n",
    "                print(\"AutoML Starting \")\n",
    "                try:\n",
    "                    aml = H2OAutoML(max_models = int(conf['max_models_fit']),\n",
    "                                    max_runtime_secs =conf['max_model_runtime'],\n",
    "                                    seed = 1234,\n",
    "                                    stopping_metric=conf['stopping_measure'],\n",
    "                                    balance_classes=True,\n",
    "                                    nfolds=conf['nfolds'],\n",
    "                                    include_algos = conf['include_algos'])\n",
    "\n",
    "                    aml.train(  x = x,\n",
    "                                y = y,\n",
    "                                training_frame =train,\n",
    "                                validation_frame=valid)\n",
    "\n",
    "                    ###Uncomment below to enable scoring\n",
    "                    model_path = conf['model_folder']\n",
    "\n",
    "                    model_name = h2o.save_model(aml.leader,\n",
    "                                                path= model_path,\n",
    "                                                force=True)\n",
    "\n",
    "\n",
    "                    # c_w.write(f'ndicate/model/dtypes_{aml.leader.model_id}.json',training_types,'processing')  ##USED WHERE MODEL HAS SPECIFIC DTYPES PER FEATURE\n",
    "\n",
    "                    print(f\"\\n\\n\\t\\t\\t\\t\\t{aml.leader.model_id}\\n\\n\")\n",
    "\n",
    "                    return(True)\n",
    "                except Exception as E:\n",
    "                    print(f\"Failiure Training for {target_name} \")\n",
    "                    print(E)\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"Not enough training records to build: {target_name}\")\n",
    "                return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "0: Ok lar... Joking wif u oni...\n",
      "\n",
      "1: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "0: U dun say so early hor... U c already then say...\n",
      "\n",
      "0: Nah I don't think he goes to usf, he lives around here though\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Format Data\n",
    "f = open(\"data/smsspamcollection/SMSSpamCollection\", \"r\")\n",
    "targets = []\n",
    "texts = []\n",
    "for line in f:\n",
    "    columns = line.split(\"\\t\", 1)\n",
    "    texts.append(columns[1])\n",
    "    if columns[0] == \"ham\":\n",
    "        targets.append(0)\n",
    "    elif columns[0] == \"spam\":\n",
    "        targets.append(1)\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        break\n",
    "# for i in range(5):\n",
    "#     print(\"{}: {}\".format(targets[i], texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  Go until jurong point, crazy.. Available only ...\n",
       "1       0                    Ok lar... Joking wif u oni...\\n\n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       0  U dun say so early hor... U c already then say...\n",
       "4       0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"target\"] = targets\n",
    "df[\"text\"] = texts\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start cleaning\n",
      "cleaning done\n",
      "Dropping english\n",
      "Dropping stop\n",
      "Stemming\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>point crazi n great world la e buffet got wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>lar u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>entri win final st receiv entri appli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dun say earli c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>dont think goe around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                           text\n",
       "0       0  point crazi n great world la e buffet got wat\n",
       "1       0                                          lar u\n",
       "2       1          entri win final st receiv entri appli\n",
       "3       0                    dun say earli c alreadi say\n",
       "4       0                   dont think goe around though"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field = \"text\"\n",
    "text_only = clean(df, text_field)\n",
    "text_only = remove_non_english(text_only, text_field)\n",
    "dropped_stop_text = drop_stop_df(text_only, text_field)\n",
    "stemmed_text = stem_df(dropped_stop_text, text_field, \"english\")\n",
    "\n",
    "stemmed_text.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"text_field\": \"text\",\n",
    "#     \"id_fields\" : [BI_ID, audio_id],\n",
    "\t\"segment\": True,\n",
    "\t\"low_memory\": False,\n",
    "    \"train_perc\": 0.9,\n",
    "\t\"min_retrain_cnt\": 50,\n",
    "\t\"min_train_records\": 10, \n",
    "    \"train_perc\": 0.8,\n",
    "    \"model_train_start_date\": \"1900-01-01\",\n",
    "    \"max_models_fit\": 5,\n",
    "    \"max_model_runtime\": 3600,\n",
    "    \"nfolds\": 0,\n",
    "    \"accuracy_measure\": \"AUC\",\n",
    "    \"stopping_measure\": \"AUC\",\n",
    "    \"include_algos\": [\"GLM\", \"DRF\", \"GBM\"],\n",
    "    \"exclude_algos\": [\"StackedEnsemble\", \"XGBoost\", \"DeepLearning\"],\n",
    "    \"model_folder\": \"/data/models/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training for Spam\n",
      "H2O session _sid_b114 closed.\n",
      "Checking whether there is an H2O instance running at http://localhost:54321 .."
     ]
    }
   ],
   "source": [
    "print(f\"Starting Training for Spam\")\n",
    "train_df = stemmed_text.loc[~stemmed_text[\"target\"].isna()]\n",
    "features, tfidf = apply_tfidf(train_df[conf['text_field']])\n",
    "tfidf_df = pd.DataFrame(features.todense())\n",
    "if tfidf_df.shape[0] != train_df.shape[0]:\n",
    "    print(\"Mis match on training data sizes\")\n",
    "    print(\"TFIDF Shape {}\".format(tfidf_df.shape))\n",
    "    print(\"Train Shape {}\".format(train_df.shape))\n",
    "else:\n",
    "    features_df = pd.merge(tfidf_df, train_df.reset_index(drop=True), left_index=True, right_index=True).copy()  \n",
    "    training_completed = build(features_df, train_df[\"target\"], \"target\", conf)\n",
    "    if training_completed:   \n",
    "        print(\"Completed Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CampaignNumber=3\n",
    "# Client = 'Affinity'\n",
    "# BI_ID = 'id'\n",
    "# audio_id = 'audio_file'\n",
    "# text_field = 'transcript'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Manually remove targets if required\n",
    "# sub_training_df = sub_training_df.drop(sub_training_df.columns[[1, 2, 3, 4]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown h2o after build\n",
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
